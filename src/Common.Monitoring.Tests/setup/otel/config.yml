receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  prometheus:
    config:
      scrape_configs:
        - job_name: 'mimir'
          static_configs:
            - targets: ['mimir:9009']
              labels:
                service: 'mimir'
                group: 'infrastructure'

        - job_name: 'loki'
          static_configs:
            - targets: ['loki:3100']
              labels:
                service: 'loki'
                group: 'infrastructure'

        - job_name: 'tempo'
          static_configs:
            - targets: ['tempo:3200']
              labels:
                service: 'tempo'
                group: 'infrastructure'

        - job_name: 'grafana'
          static_configs:
            - targets: ['grafana:3000']
              labels:
                service: 'grafana'
                group: 'infrastructure'

processors:
  batch:
    timeout: 1s

  tail_sampling:
    decision_wait: 30s    # The time to wait for a decision to be made.
    # The following policies follow a logical OR pattern, meaning that if any of the policies match,
    # the trace will be kept. For logical AND, you can use the `and` policy. Every span of a trace is
    # examined by each policy in turn. A match will cause a short-circuit.
    policies: [
      # This policy defines that traces that include spans that contain errors should be kept.
      {
        name: sample-erroring-traces,           # Name of the policy.
        type: status_code,                      # The type must match the type of policy to be used.
        status_code: { status_codes: [ERROR] }  # Only sample traces which have a span containing an error.
      },
      # This policy defines that traces that are over 200ms should be sampled.
      {
        name: sample-long-traces,               # Name of the policy.
        type: latency,                          # The type must match the type of policy to be used.
        latency: { threshold_ms: 200 },         # Only sample traces which are longer than 200ms in duration.
      },
    ]

connectors:
  spanmetrics:
    namespace: traces.spanmetrics   # Prefix all metrics with `traces.spanmetrics` (this becomes `traces_spanmetrics`).
    # Determine the type of histogram to use for span metrics.
    histogram:
      explicit:                     # Explicit histograms have pre-defined bucket sizes (use default here).
    # Defines additional label dimensions of the metrics from trace span attributes present.
    dimensions:
      - name: http.method
      - name: http.target
      - name: http.status_code
      - name: service.version
    # Ensure exemplars are enabled and sent to the metrics store.
    exemplars:
      enabled: true

  # The servicegraph connector is used to output service node metrics based on received trace spans.
  servicegraph:
    # Defines which exporter the processor will write metrics to.
    metrics_exporter: prometheusremotewrite
    # Defines additional label dimensions of the metrics from trace span attributes present.
    store:                      # Configuration for the in-memory store.
      ttl: 2s                   # Time to wait for an edge to be completed.
      max_items: 200            # Number of edges that will be stored in the storeMap.
    cache_loop: 2m              # The timeout used to clean the cache periodically.
    store_expiration_loop: 10s  # The timeout used to expire old entries from the store periodically.
    # Virtual node peer attributes allow server nodes to be generated where instrumentation isn't present (eg. where
    # service client calls remotely to a service that does not include instrumentation).
    # Service nodes/edges will be generated for any attribute defined.
    virtual_node_peer_attributes:
      - db.name


# Define exporters to data stores.
# See https://opentelemetry.io/docs/collector/configuration/#exporters
# Also see https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor#recommended-processors
exporters:
  # Exporter for sending trace data to Tempo.
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true

  prometheusremotewrite:
    endpoint: http://mimir:9009/api/v1/push
    tls:
      insecure: true

  otlp/loki:
    endpoint: loki:3100
    tls:
      insecure: true

extensions:
  health_check:
  pprof:
  zpages:

service:
  extensions: [health_check, pprof, zpages]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp/tempo]

    metrics:
      receivers: [prometheus, otlp]
      processors: [batch]
      exporters: [prometheusremotewrite]

    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp/loki]
